{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# locate interpreter\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:\\\\Users\\\\epicj\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe'\n",
    "\n",
    "# build session and define configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirportAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# set up the structure for passengerData dataframe\n",
    "passengerDataSchema = StructType([\n",
    "    StructField(\"PassengerID\", StringType(), True),\n",
    "    StructField(\"FlightID\", StringType(), True),\n",
    "    StructField(\"DepartureAirportID\", StringType(), True),\n",
    "    StructField(\"DestinationAirportID\", StringType(), True),\n",
    "    StructField(\"DepartureTime\", IntegerType(), True),\n",
    "    StructField(\"FlightDuration\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# read \"AComp_Passenger_data_no_error.csv\" dataset into pyspark dataframe using its schema\n",
    "passengerData = spark.read.csv('AComp_Passenger_data_no_error.csv',schema=passengerDataSchema, header=False)\n",
    "\n",
    "# remove duplicate flights\n",
    "passengerDataNoDupe = passengerData.dropDuplicates([\"FlightID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda operations are extended and modified code from:\n",
    "# https://cs.stanford.edu/people/nick/py/python-map-lambda.html\n",
    "# https://python-course.eu/advanced-python/lambda-filter-reduce-map.php\n",
    "\n",
    "# map airports from passengerData dataframe and sort for clarity\n",
    "mapping = passengerDataNoDupe.select(\"DepartureAirportID\").rdd.map(lambda x: (x[0], 1))\n",
    "sorted_mapping = mapping.sortByKey()\n",
    "\n",
    "# group mapped data by airport and count the occurrences of each\n",
    "groupedMapping = sorted_mapping.groupByKey().map(lambda l: (l[0], sum(l[1])))\n",
    "\n",
    "# set up the structure for flightCounts dataframe\n",
    "flightCountsSchema = StructType([\n",
    "    StructField(\"AirportID\", StringType(), True),\n",
    "    StructField(\"Number of Flights\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# create dataframe for flight counts from groupedMapping\n",
    "flightCountsDF = spark.createDataFrame(groupedMapping, flightCountsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+---------+\n",
      "|AirportName|AirportID| Latitude|Longitude|\n",
      "+-----------+---------+---------+---------+\n",
      "|    ATLANTA|      ATL| 33.63672|-84.42807|\n",
      "|    BEIJING|      PEK|40.080112|116.58456|\n",
      "|     LONDON|      LHR|  51.4775|-0.461389|\n",
      "|    CHICAGO|      ORD|41.978603|-87.90484|\n",
      "|      TOKYO|      HND|35.552258| 139.7797|\n",
      "+-----------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up the structure for airportsData dataframe\n",
    "airportsDataSchema = StructType([\n",
    "    StructField(\"AirportName\", StringType(), True),\n",
    "    StructField(\"AirportID\", StringType(), True),\n",
    "    StructField(\"Latitude\", FloatType(), True),\n",
    "    StructField(\"Longitude\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# read 'Top30_airports_LatLong.csv' dataset into pyspark dataframe using its schema\n",
    "airportsData = spark.read.csv('Top30_airports_LatLong.csv', schema = airportsDataSchema, header=False)\n",
    "airportsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|AirportID|Number of Flights|\n",
      "+---------+-----------------+\n",
      "|      AMS|                1|\n",
      "|      CAN|                2|\n",
      "|      CGK|                2|\n",
      "|      CLT|                1|\n",
      "|      DEN|                3|\n",
      "|      DFW|                1|\n",
      "|      FCO|                1|\n",
      "|      HND|                1|\n",
      "|      IAH|                2|\n",
      "|      JFK|                1|\n",
      "|      KUL|                2|\n",
      "|      MIA|                1|\n",
      "|      MUC|                1|\n",
      "|      ORD|                2|\n",
      "|      PEK|                1|\n",
      "|      PVG|                1|\n",
      "|      ATL|                2|\n",
      "|      BKK|                1|\n",
      "|      CDG|                1|\n",
      "|      LAS|                1|\n",
      "+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "used airports:  ['AMS', 'CAN', 'CGK', 'CLT', 'DEN', 'DFW', 'FCO', 'HND', 'IAH', 'JFK', 'KUL', 'MIA', 'MUC', 'ORD', 'PEK', 'PVG', 'ATL', 'BKK', 'CDG', 'LAS', 'LHR', 'MAD']\n",
      "unused airports:  ['LAX', 'FRA', 'HKG', 'DXB', 'SIN', 'SFO', 'PHX', 'IST']\n"
     ]
    }
   ],
   "source": [
    "# find used airports: all airports in flightCountsDF\n",
    "usedAirports = flightCountsDF.rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "# find all airports: all airports in airportsData\n",
    "allAirports = airportsData.select(\"AirportID\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# extended and modified code from: https://stackoverflow.com/questions/44192279/find-the-list-values-not-in-pandas-dataframe-data\n",
    "# find unused airports: the difference between the two lists above\n",
    "unusedAirports = [x for x in allAirports if x not in usedAirports]\n",
    "\n",
    "# output information as per brief\n",
    "flightCountsDF.show()\n",
    "print(\"used airports: \", usedAirports)\n",
    "print(\"unused airports: \", unusedAirports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|AirportID|Number of Flights|\n",
      "+---------+-----------------+\n",
      "|      AMS|                1|\n",
      "|      CAN|                2|\n",
      "|      CGK|                2|\n",
      "|      CLT|                1|\n",
      "|      DEN|                3|\n",
      "|      DFW|                1|\n",
      "|      FCO|                1|\n",
      "|      HND|                1|\n",
      "|      IAH|                2|\n",
      "|      JFK|                1|\n",
      "|      KUL|                2|\n",
      "|      MIA|                1|\n",
      "|      MUC|                1|\n",
      "|      ORD|                2|\n",
      "|      PEK|                1|\n",
      "|      PVG|                1|\n",
      "|      ATL|                2|\n",
      "|      BKK|                1|\n",
      "|      CDG|                1|\n",
      "|      LAS|                1|\n",
      "+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Used airports:  ['AMS', 'CAN', 'CGK', 'CLT', 'DEN', 'DFW', 'FCO', 'HND', 'IAH', 'JFK', 'KUL', 'MIA', 'MUC', 'ORD', 'PEK', 'PVG', 'ATL', 'BKK', 'CDG', 'LAS', 'LHR', 'MAD']\n",
      "Unused airports:  ['FRA', 'IST', 'SIN', 'DXB', 'PHX', 'SFO', 'HKG', 'LAX']\n"
     ]
    }
   ],
   "source": [
    "# Extract AirportIDs from used airports\n",
    "usedAirports = [row['AirportID'] for row in flightCountsDF.select(\"AirportID\").collect()]\n",
    "\n",
    "# Extract AirportIDs from all airports\n",
    "allAirports = [row['AirportID'] for row in airportsData.select(\"AirportID\").distinct().collect()]\n",
    "\n",
    "# extended and modified code from: https://stackoverflow.com/questions/44192279/find-the-list-values-not-in-pandas-dataframe-data\n",
    "# find unused airports: the difference between the two lists of AirportIDs\n",
    "unusedAirports = [x for x in allAirports if x not in usedAirports]\n",
    "\n",
    "# output information as per brief\n",
    "flightCountsDF.show()\n",
    "# output information\n",
    "print(\"Used airports: \", usedAirports)\n",
    "print(\"Unused airports: \", unusedAirports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ATT7791R', 15), ('DKZ3042O', 11), ('FYL5866L', 20), ('GMO5938W', 25), ('MBA8071P', 16), ('MOO1786A', 13), ('QHU1140O', 21), ('RPG3351U', 13), ('RUM0422W', 14), ('SOH3431A', 18), ('TMV7633W', 15), ('ULZ8130D', 27), ('VYU9214I', 15), ('XIL3623J', 13), ('XOY7948U', 16), ('BER7172M', 17), ('DAU2617A', 12), ('EWH6301Y', 10), ('HUR0974O', 7), ('HZT2506M', 14), ('JVY9791G', 20), ('KJR6646J', 23), ('PME8178S', 18), ('SQU6245R', 21), ('VDC9164W', 15), ('VYW5940P', 17), ('WPW9201U', 11), ('WSK1289Z', 21), ('XXQ4064B', 25), ('YZO4444S', 17)]\n"
     ]
    }
   ],
   "source": [
    "# map flight IDs from passengerData and sort for clarity\n",
    "passengerCountMap = passengerData.select(\"FlightID\").rdd.map(lambda x: (x[0], 1))\n",
    "passengerCountMapSorted = passengerCountMap.sortByKey()\n",
    "\n",
    "# group mapped data by flight and count occurences for each\n",
    "groupedPassengerCountMap = passengerCountMapSorted.groupByKey().map(lambda l: (l[0], sum(l[1])))\n",
    "\n",
    "# collect the passenger count data into a list\n",
    "passengerCountData = groupedPassengerCountMap.collect()\n",
    "print(passengerCountData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------+--------------------+-------------+--------------+\n",
      "|FlightID|PassengerCount|DepartureAirportID|DestinationAirportID|DepartureTime|FlightDuration|\n",
      "+--------+--------------+------------------+--------------------+-------------+--------------+\n",
      "|ATT7791R|            15|               AMS|                 DEN|   1420564394|          1001|\n",
      "|BER7172M|            17|               KUL|                 LAS|   1420565167|          1848|\n",
      "|DAU2617A|            12|               CGK|                 SFO|   1420564986|          1811|\n",
      "|DKZ3042O|            11|               MIA|                 SFO|   1420563927|           538|\n",
      "|EWH6301Y|            10|               CAN|                 DFW|   1420564967|          1683|\n",
      "+--------+--------------+------------------+--------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe for passengerCounts from passengerCountData\n",
    "passengerCountsDF = spark.createDataFrame(passengerCountData, schema=[\"FlightID\", \"PassengerCount\"])\n",
    "\n",
    "# create a copy of passengerDataNoDupe, drop PassengerID and sort for clarity\n",
    "flightsInfoDF = passengerDataNoDupe.select(\"FlightID\", \"DepartureAirportID\", \"DestinationAirportID\", \"DepartureTime\", \"FlightDuration\")\n",
    "flightsInfoDF = flightsInfoDF.orderBy(\"FlightID\")\n",
    "\n",
    "# merge the two dataframes by FlightID\n",
    "flightsInfoDFMerged = flightsInfoDF.join(passengerCountsDF, \"FlightID\", \"left\")\n",
    "\n",
    "# rearrange columns as per the requirement\n",
    "flightsInfoDFMerged = flightsInfoDFMerged.select(\"FlightID\", \"PassengerCount\", \"DepartureAirportID\", \"DestinationAirportID\", \"DepartureTime\", \"FlightDuration\")\n",
    "\n",
    "# only show 5 for speed\n",
    "flightsInfoDFMerged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------+--------------------+-------------+--------------+-----------+\n",
      "|FlightID|PassengerCount|DepartureAirportID|DestinationAirportID|DepartureTime|FlightDuration|ArrivalTime|\n",
      "+--------+--------------+------------------+--------------------+-------------+--------------+-----------+\n",
      "|ATT7791R|            15|               AMS|                 DEN|   1420564394|          1001| 1420565395|\n",
      "|BER7172M|            17|               KUL|                 LAS|   1420565167|          1848| 1420567015|\n",
      "|DAU2617A|            12|               CGK|                 SFO|   1420564986|          1811| 1420566797|\n",
      "|DKZ3042O|            11|               MIA|                 SFO|   1420563927|           538| 1420564465|\n",
      "|EWH6301Y|            10|               CAN|                 DFW|   1420564967|          1683| 1420566650|\n",
      "+--------+--------------+------------------+--------------------+-------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# calculate arrivalTime by adding departureTime to flightDuration\n",
    "flightsInfoDFMerged = flightsInfoDFMerged.withColumn(\"ArrivalTime\", col(\"FlightDuration\") + col(\"DepartureTime\"))\n",
    "\n",
    "# show 5 for speed\n",
    "flightsInfoDFMerged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+\n",
      "|FlightID|PassengerCount|DepartureAirportID|DestinationAirportID|      DepartureTime|FlightDuration|        ArrivalTime|\n",
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+\n",
      "|ATT7791R|            15|               AMS|                 DEN|2015-01-06 17:13:14|          1001|2015-01-06 17:29:55|\n",
      "|BER7172M|            17|               KUL|                 LAS|2015-01-06 17:26:07|          1848|2015-01-06 17:56:55|\n",
      "|DAU2617A|            12|               CGK|                 SFO|2015-01-06 17:23:06|          1811|2015-01-06 17:53:17|\n",
      "|DKZ3042O|            11|               MIA|                 SFO|2015-01-06 17:05:27|           538|2015-01-06 17:14:25|\n",
      "|EWH6301Y|            10|               CAN|                 DFW|2015-01-06 17:22:47|          1683|2015-01-06 17:50:50|\n",
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# convert epoch times in departureTime and arrivalTime columns to more readable date/time format\n",
    "flightsInfoDFMerged = flightsInfoDFMerged.withColumn(\"DepartureTime\", from_unixtime(col(\"DepartureTime\")))\n",
    "flightsInfoDFMerged = flightsInfoDFMerged.withColumn(\"ArrivalTime\", from_unixtime(col(\"ArrivalTime\")))\n",
    "\n",
    "# show 5 for speed\n",
    "flightsInfoDFMerged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ATT7791R', 15, 'AMS', 'DEN', '2015-01-06 17:13:14', 1001, '2015-01-06 17:29:55'), ('BER7172M', 17, 'KUL', 'LAS', '2015-01-06 17:26:07', 1848, '2015-01-06 17:56:55'), ('DAU2617A', 12, 'CGK', 'SFO', '2015-01-06 17:23:06', 1811, '2015-01-06 17:53:17'), ('DKZ3042O', 11, 'MIA', 'SFO', '2015-01-06 17:05:27', 538, '2015-01-06 17:14:25'), ('EWH6301Y', 10, 'CAN', 'DFW', '2015-01-06 17:22:47', 1683, '2015-01-06 17:50:50'), ('FYL5866L', 20, 'ATL', 'HKG', '2015-01-06 17:25:40', 1751, '2015-01-06 17:54:51'), ('GMO5938W', 25, 'LHR', 'PEK', '2015-01-06 17:11:57', 1057, '2015-01-06 17:29:34'), ('HUR0974O', 7, 'DEN', 'PVG', '2015-01-06 17:15:25', 1398, '2015-01-06 17:38:43'), ('HZT2506M', 14, 'IAH', 'AMS', '2015-01-06 17:12:04', 1044, '2015-01-06 17:29:28'), ('JVY9791G', 20, 'PVG', 'FCO', '2015-01-06 17:16:01', 1189, '2015-01-06 17:35:50'), ('KJR6646J', 23, 'IAH', 'BKK', '2015-01-06 17:26:43', 1928, '2015-01-06 17:58:51'), ('MBA8071P', 16, 'KUL', 'PEK', '2015-01-06 17:04:16', 572, '2015-01-06 17:13:48'), ('MOO1786A', 13, 'MAD', 'FRA', '2015-01-06 16:56:48', 184, '2015-01-06 16:59:52'), ('PME8178S', 18, 'DEN', 'PEK', '2015-01-06 17:13:29', 1322, '2015-01-06 17:35:31'), ('QHU1140O', 21, 'CDG', 'LAS', '2015-01-06 17:14:58', 1133, '2015-01-06 17:33:51'), ('RPG3351U', 13, 'HND', 'CAN', '2015-01-06 16:59:29', 374, '2015-01-06 17:05:43'), ('RUM0422W', 14, 'MUC', 'MAD', '2015-01-06 16:58:59', 194, '2015-01-06 17:02:13'), ('SOH3431A', 18, 'ORD', 'MIA', '2015-01-06 17:00:49', 250, '2015-01-06 17:04:59'), ('SQU6245R', 21, 'DEN', 'FRA', '2015-01-06 17:14:20', 1049, '2015-01-06 17:31:49'), ('TMV7633W', 15, 'CGK', 'DXB', '2015-01-06 17:05:58', 849, '2015-01-06 17:20:07'), ('ULZ8130D', 27, 'CAN', 'DFW', '2015-01-06 17:23:03', 1683, '2015-01-06 17:51:06'), ('VDC9164W', 15, 'FCO', 'LAS', '2015-01-06 17:18:18', 1276, '2015-01-06 17:39:34'), ('VYU9214I', 15, 'ORD', 'DXB', '2015-01-06 17:18:36', 1510, '2015-01-06 17:43:46'), ('VYW5940P', 17, 'LAS', 'SIN', '2015-01-06 17:26:43', 1843, '2015-01-06 17:57:26'), ('WPW9201U', 11, 'DFW', 'PEK', '2015-01-06 17:21:09', 1452, '2015-01-06 17:45:21'), ('WSK1289Z', 21, 'CLT', 'DEN', '2015-01-06 16:59:02', 278, '2015-01-06 17:03:40'), ('XIL3623J', 13, 'PEK', 'LAX', '2015-01-06 17:13:34', 1302, '2015-01-06 17:35:16'), ('XOY7948U', 16, 'ATL', 'LHR', '2015-01-06 17:07:18', 877, '2015-01-06 17:21:55'), ('XXQ4064B', 25, 'JFK', 'FRA', '2015-01-06 17:05:17', 802, '2015-01-06 17:18:39'), ('YZO4444S', 17, 'BKK', 'MIA', '2015-01-06 17:28:50', 2027, '2015-01-06 18:02:37')]\n"
     ]
    }
   ],
   "source": [
    "# collect rows from flightsInfoDFMerged\n",
    "flightRecords = flightsInfoDFMerged.collect()\n",
    "\n",
    "# convert the collected rows to a tuple list\n",
    "finalFlightList = [tuple(row) for row in flightRecords]\n",
    "\n",
    "print(finalFlightList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# convert airportsData into a tuple list the same way as before only selecting necessary columns\n",
    "airportsRecords = airportsData.select(\"AirportID\", \"Latitude\", \"Longitude\").collect()\n",
    "airportsRecordsList = [tuple(row) for row in airportsRecords]\n",
    "\n",
    "# empty list for distances of flights\n",
    "distancesData = []\n",
    "\n",
    "# iterate through each flight\n",
    "for flight in finalFlightList:\n",
    "\n",
    "    # assign flightID, depAirport and desAirport for clarity\n",
    "    flightID = flight[0]\n",
    "    depAirport = flight[2]\n",
    "    desAirport = flight[3]\n",
    "\n",
    "    # iterate through airportsRecordsList to find the departure airport's information\n",
    "    depAirportInfo = next((airport for airport in airportsRecordsList if airport[0] == depAirport), None)\n",
    "    # iterate through airportsRecordsList to find the destination airport's information\n",
    "    desAirportInfo = next((airport for airport in airportsRecordsList if airport[0] == desAirport), None)\n",
    "\n",
    "    # check to ensure that we have all the information we need\n",
    "    if depAirportInfo and desAirportInfo:\n",
    "\n",
    "        # get latitude and longitude for departure and destination airports and simulateneously convert them into radians\n",
    "        depLat, depLon = math.radians(depAirportInfo[1]), math.radians(depAirportInfo[2])\n",
    "        desLat, desLon = math.radians(desAirportInfo[1]), math.radians(desAirportInfo[2])\n",
    "\n",
    "        # calculate distance using Haversine formula\n",
    "        # haversine formula from https://www.movable-type.co.uk/scripts/latlong.html\n",
    "        diffLat = desLat - depLat\n",
    "        diffLon = desLon - depLon\n",
    "        a = math.sin(diffLat / 2) ** 2 + math.cos(depLat) * math.cos(desLat) * math.sin(diffLon / 2) ** 2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "        # multiply by Earth's radius to get distances in nautical miles\n",
    "        distNM = 3440.065 * c\n",
    "\n",
    "        # append distance to distances list\n",
    "        distancesData.append([flightID, distNM])\n",
    "    else:\n",
    "        print(\"Info missing for \", depAirport, \"or\", desAirport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|FlightID|          Distance|\n",
      "+--------+------------------+\n",
      "|ATT7791R| 4169.868902347976|\n",
      "|BER7172M| 7693.953583856841|\n",
      "|DAU2617A| 7538.467918971757|\n",
      "|DKZ3042O| 2242.815642331329|\n",
      "|EWH6301Y|7007.3349391155825|\n",
      "|FYL5866L|7288.2440255521215|\n",
      "|GMO5938W| 4402.109680576151|\n",
      "|HUR0974O| 5820.650229676103|\n",
      "|HZT2506M| 4346.269663648936|\n",
      "|JVY9791G| 4950.959340864341|\n",
      "|KJR6646J| 8025.213556825649|\n",
      "|MBA8071P|2383.0795801155564|\n",
      "|MOO1786A|  765.880606529188|\n",
      "|PME8178S| 5502.890889477019|\n",
      "|QHU1140O|   4717.5544852904|\n",
      "|RPG3351U| 1557.699990276518|\n",
      "|RUM0422W| 807.5904051437628|\n",
      "|SOH3431A|1042.1373565606136|\n",
      "|SQU6245R| 4367.076218559275|\n",
      "|TMV7633W| 3535.134495309536|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create RDD from distances list to utilize cluster computing for speed\n",
    "rdd = spark.sparkContext.parallelize(distancesData)\n",
    "rddRows = rdd.map(lambda x: Row(FlightID=x[0], Distance=x[1]))\n",
    "\n",
    "# create distances dataframe from RDD of rows\n",
    "distancesDF = spark.createDataFrame(rddRows)\n",
    "\n",
    "distancesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+------------------+\n",
      "|FlightID|PassengerCount|DepartureAirportID|DestinationAirportID|      DepartureTime|FlightDuration|        ArrivalTime|          Distance|\n",
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+------------------+\n",
      "|ATT7791R|            15|               AMS|                 DEN|2015-01-06 17:13:14|          1001|2015-01-06 17:29:55| 4169.868902347976|\n",
      "|BER7172M|            17|               KUL|                 LAS|2015-01-06 17:26:07|          1848|2015-01-06 17:56:55| 7693.953583856841|\n",
      "|DAU2617A|            12|               CGK|                 SFO|2015-01-06 17:23:06|          1811|2015-01-06 17:53:17| 7538.467918971757|\n",
      "|DKZ3042O|            11|               MIA|                 SFO|2015-01-06 17:05:27|           538|2015-01-06 17:14:25| 2242.815642331329|\n",
      "|EWH6301Y|            10|               CAN|                 DFW|2015-01-06 17:22:47|          1683|2015-01-06 17:50:50|7007.3349391155825|\n",
      "|FYL5866L|            20|               ATL|                 HKG|2015-01-06 17:25:40|          1751|2015-01-06 17:54:51|7288.2440255521215|\n",
      "|GMO5938W|            25|               LHR|                 PEK|2015-01-06 17:11:57|          1057|2015-01-06 17:29:34| 4402.109680576151|\n",
      "|HUR0974O|             7|               DEN|                 PVG|2015-01-06 17:15:25|          1398|2015-01-06 17:38:43| 5820.650229676103|\n",
      "|HZT2506M|            14|               IAH|                 AMS|2015-01-06 17:12:04|          1044|2015-01-06 17:29:28| 4346.269663648936|\n",
      "|JVY9791G|            20|               PVG|                 FCO|2015-01-06 17:16:01|          1189|2015-01-06 17:35:50| 4950.959340864341|\n",
      "|KJR6646J|            23|               IAH|                 BKK|2015-01-06 17:26:43|          1928|2015-01-06 17:58:51| 8025.213556825649|\n",
      "|MBA8071P|            16|               KUL|                 PEK|2015-01-06 17:04:16|           572|2015-01-06 17:13:48|2383.0795801155564|\n",
      "|MOO1786A|            13|               MAD|                 FRA|2015-01-06 16:56:48|           184|2015-01-06 16:59:52|  765.880606529188|\n",
      "|PME8178S|            18|               DEN|                 PEK|2015-01-06 17:13:29|          1322|2015-01-06 17:35:31| 5502.890889477019|\n",
      "|QHU1140O|            21|               CDG|                 LAS|2015-01-06 17:14:58|          1133|2015-01-06 17:33:51|   4717.5544852904|\n",
      "|RPG3351U|            13|               HND|                 CAN|2015-01-06 16:59:29|           374|2015-01-06 17:05:43| 1557.699990276518|\n",
      "|RUM0422W|            14|               MUC|                 MAD|2015-01-06 16:58:59|           194|2015-01-06 17:02:13| 807.5904051437628|\n",
      "|SOH3431A|            18|               ORD|                 MIA|2015-01-06 17:00:49|           250|2015-01-06 17:04:59|1042.1373565606136|\n",
      "|SQU6245R|            21|               DEN|                 FRA|2015-01-06 17:14:20|          1049|2015-01-06 17:31:49| 4367.076218559275|\n",
      "|TMV7633W|            15|               CGK|                 DXB|2015-01-06 17:05:58|           849|2015-01-06 17:20:07| 3535.134495309536|\n",
      "+--------+--------------+------------------+--------------------+-------------------+--------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge the distances dataframe with main flight info dataframe\n",
    "flightsInfoDFMergedWithDist = flightsInfoDFMerged.join(distancesDF, on='FlightID', how='left')\n",
    "\n",
    "flightsInfoDFMergedWithDist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // TECHNIQUE a \\\\\n",
    "\n",
    "# create dictionary to hold each passengers' flights by their ID\n",
    "passengerFlightDict = {}\n",
    "\n",
    "# collect PassengerID and FlightID columns from passengerData into a new dataframe\n",
    "passengerFlights = passengerData.select(\"PassengerID\", \"FlightID\").collect()\n",
    "\n",
    "# iterate through passengerFlights list to populate passengerFlightDict\n",
    "for row in passengerFlights:\n",
    "    passengerID = row[\"PassengerID\"]\n",
    "    flightID = row[\"FlightID\"]\n",
    "    if passengerID in passengerFlightDict:\n",
    "        passengerFlightDict[passengerID].append(flightID)\n",
    "    else:\n",
    "        passengerFlightDict[passengerID] = [flightID]\n",
    "\n",
    "# create dictionary to hold each passengers' flight distances by their ID\n",
    "passengerDistanceDict = {}\n",
    "\n",
    "# iterate through each passenger in passengerFlightDict\n",
    "for passenger, flights in passengerFlightDict.items():\n",
    "    totalDistance = 0\n",
    "    # iterate through each flight corresponding to current passenger\n",
    "    for flightID in flights:\n",
    "        # cross-reference with flightsInfoDFMergedWithDist to find the distance per flight\n",
    "        distance = flightsInfoDFMergedWithDist.filter(col('FlightID') == flightID).select('Distance').collect()\n",
    "        # add to total distance for current passenger\n",
    "        totalDistance += distance[0]['Distance']\n",
    "\n",
    "    # assign total distance value to passenger in passengerDistanceDict\n",
    "    passengerDistanceDict[passenger] = totalDistance\n",
    "\n",
    "# create RDD from passengerDistanceDict to utilize cluster computing for speed\n",
    "passengerDistanceRDD = spark.sparkContext.parallelize(list(passengerDistanceDict.items()))\n",
    "\n",
    "# create passengerDistance dataframe from RDD\n",
    "passengerDistanceDF = passengerDistanceRDD.toDF([\"PassengerID\", \"Total Distance Flown\"])\n",
    "passengerDistanceDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|PassengerID|Total Distance Flown|\n",
      "+-----------+--------------------+\n",
      "| KKP5277HZ7|   58555.91646919763|\n",
      "| HCA3158QA6|   96993.60074894653|\n",
      "| POP2875LH3|     81031.601650141|\n",
      "| EDV2089LK5|   70430.77126985819|\n",
      "| PUD8209OG3|  115812.65362009435|\n",
      "| PAJ3974RK1|   34229.24298116658|\n",
      "| JBE2302VO4|   69002.45039215357|\n",
      "| PIT2755XC1|   36076.50494670427|\n",
      "| YMH6360YP0|   76257.83823570098|\n",
      "| HGO4350KK1|   81796.00102017862|\n",
      "| SPR4484HA6|  122258.09976971563|\n",
      "| IEG9308EA5|   42015.63813599509|\n",
      "| WYU2010YH8|    96735.8614136759|\n",
      "| CDC0302NN5|   63112.87953247823|\n",
      "| CKZ3132BR4|   92728.59102689348|\n",
      "| XFG5747ZT9|   66420.44625193973|\n",
      "| CYJ0225CH1|   54192.18061353429|\n",
      "| WTC9125IE5|   59610.49391610161|\n",
      "| LLZ3798PE3|   84096.07280138526|\n",
      "| VZY2993ME1|   73690.19854471082|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# // TECHNIQUE b \\\\\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# collect PassengerID and FlightID columns from passengerData into a DataFrame\n",
    "passengerFlights = passengerData.select(\"PassengerID\", \"FlightID\")\n",
    "\n",
    "# merge passenger flights with their distances\n",
    "passengerFlightDistance = passengerFlights.join(flightsInfoDFMergedWithDist, passengerFlights[\"FlightID\"] == flightsInfoDFMergedWithDist[\"FlightID\"], \"left_outer\")\n",
    "\n",
    "# calculate total distance flown by each passenger and store in a new dataframe and rename distance column for clarity\n",
    "passengerDistance = passengerFlightDistance.groupBy(\"PassengerID\").agg({\"Distance\": \"sum\"}).withColumnRenamed(\"sum(Distance)\", \"Total Distance Flown\")\n",
    "\n",
    "passengerDistance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger with highest total miles flown: UES9151GS5\n"
     ]
    }
   ],
   "source": [
    "# select PassengerID and Total Distance Flown and then order by distance flown in descending order\n",
    "sortedPassengers = passengerDistance.select(\"PassengerID\", \"Total Distance Flown\").orderBy(col(\"Total Distance Flown\").desc())\n",
    "\n",
    "# select the PassengerID of the first row after ordering to get the passenger with the most miles\n",
    "passengerWithMostMiles = sortedPassengers.select(\"PassengerID\").first()\n",
    "\n",
    "# display the passenger with the highest total miles flown\n",
    "print(\"Passenger with highest total miles flown:\", passengerWithMostMiles[\"PassengerID\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
